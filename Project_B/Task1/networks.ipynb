{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lk5Vl1k_ck9Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Acknowledgement to\n",
        "# https://github.com/kuangliu/pytorch-cifar,\n",
        "# https://github.com/BIGBALLON/CIFAR-ZOO,\n",
        "\n",
        "\n",
        "''' Swish activation '''\n",
        "class Swish(nn.Module): # Swish(x) = x∗σ(x)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.sigmoid(input)\n",
        "\n",
        "\n",
        "''' MLP '''\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n",
        "        self.fc_2 = nn.Linear(128, 128)\n",
        "        self.fc_3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = F.relu(self.fc_1(out))\n",
        "        out = F.relu(self.fc_2(out))\n",
        "        out = self.fc_3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "''' ConvNet '''\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n",
        "        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n",
        "        self.classifier = nn.Linear(num_feat, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def embed(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "    def _get_activation(self, net_act):\n",
        "        if net_act == 'sigmoid':\n",
        "            return nn.Sigmoid()\n",
        "        elif net_act == 'relu':\n",
        "            return nn.ReLU(inplace=True)\n",
        "        elif net_act == 'leakyrelu':\n",
        "            return nn.LeakyReLU(negative_slope=0.01)\n",
        "        elif net_act == 'swish':\n",
        "            return Swish()\n",
        "        else:\n",
        "            exit('unknown activation function: %s'%net_act)\n",
        "\n",
        "    def _get_pooling(self, net_pooling):\n",
        "        if net_pooling == 'maxpooling':\n",
        "            return nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif net_pooling == 'avgpooling':\n",
        "            return nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        elif net_pooling == 'none':\n",
        "            return None\n",
        "        else:\n",
        "            exit('unknown net_pooling: %s'%net_pooling)\n",
        "\n",
        "    def _get_normlayer(self, net_norm, shape_feat):\n",
        "        # shape_feat = (c*h*w)\n",
        "        if net_norm == 'batchnorm':\n",
        "            return nn.BatchNorm2d(shape_feat[0], affine=True)\n",
        "        elif net_norm == 'layernorm':\n",
        "            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n",
        "        elif net_norm == 'instancenorm':\n",
        "            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n",
        "        elif net_norm == 'groupnorm':\n",
        "            return nn.GroupNorm(4, shape_feat[0], affine=True)\n",
        "        elif net_norm == 'none':\n",
        "            return None\n",
        "        else:\n",
        "            exit('unknown net_norm: %s'%net_norm)\n",
        "\n",
        "    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n",
        "        layers = []\n",
        "        in_channels = channel\n",
        "        if im_size[0] == 28:\n",
        "            im_size = (32, 32)\n",
        "        shape_feat = [in_channels, im_size[0], im_size[1]]\n",
        "        for d in range(net_depth):\n",
        "            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n",
        "            shape_feat[0] = net_width\n",
        "            if net_norm != 'none':\n",
        "                layers += [self._get_normlayer(net_norm, shape_feat)]\n",
        "            layers += [self._get_activation(net_act)]\n",
        "            in_channels = net_width\n",
        "            if net_pooling != 'none':\n",
        "                layers += [self._get_pooling(net_pooling)]\n",
        "                shape_feat[1] //= 2\n",
        "                shape_feat[2] //= 2\n",
        "\n",
        "        return nn.Sequential(*layers), shape_feat\n",
        "\n",
        "\n",
        "\n",
        "''' LeNet '''\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc_2 = nn.Linear(120, 84)\n",
        "        self.fc_3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = F.relu(self.fc_2(x))\n",
        "        x = self.fc_3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "''' AlexNet '''\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "''' AlexNetBN '''\n",
        "class AlexNetBN(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(AlexNetBN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "''' VGG '''\n",
        "cfg_vgg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name, channel, num_classes, norm='instancenorm'):\n",
        "        super(VGG, self).__init__()\n",
        "        self.channel = channel\n",
        "        self.features = self._make_layers(cfg_vgg[vgg_name], norm)\n",
        "        self.classifier = nn.Linear(512 if vgg_name != 'VGGS' else 128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def _make_layers(self, cfg, norm):\n",
        "        layers = []\n",
        "        in_channels = self.channel\n",
        "        for ic, x in enumerate(cfg):\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=3 if self.channel==1 and ic==0 else 1),\n",
        "                           nn.GroupNorm(x, x, affine=True) if norm=='instancenorm' else nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def VGG11(channel, num_classes):\n",
        "    return VGG('VGG11', channel, num_classes)\n",
        "def VGG11BN(channel, num_classes):\n",
        "    return VGG('VGG11', channel, num_classes, norm='batchnorm')\n",
        "def VGG13(channel, num_classes):\n",
        "    return VGG('VGG13', channel, num_classes)\n",
        "def VGG16(channel, num_classes):\n",
        "    return VGG('VGG16', channel, num_classes)\n",
        "def VGG19(channel, num_classes):\n",
        "    return VGG('VGG19', channel, num_classes)\n",
        "\n",
        "\n",
        "''' ResNet_AP '''\n",
        "# The conv(stride=2) is replaced by conv(stride=1) + avgpool(kernel_size=2, stride=2)\n",
        "\n",
        "class BasicBlock_AP(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
        "        super(BasicBlock_AP, self).__init__()\n",
        "        self.norm = norm\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n",
        "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n",
        "                nn.AvgPool2d(kernel_size=2, stride=2), # modification\n",
        "                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        if self.stride != 1: # modification\n",
        "            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck_AP(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
        "        super(Bottleneck_AP, self).__init__()\n",
        "        self.norm = norm\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n",
        "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n",
        "                nn.AvgPool2d(kernel_size=2, stride=2),  # modification\n",
        "                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        if self.stride != 1: # modification\n",
        "            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_AP(nn.Module):\n",
        "    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n",
        "        super(ResNet_AP, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.norm = norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.classifier = nn.Linear(512 * block.expansion * 3 * 3 if channel==1 else 512 * block.expansion * 4 * 4, num_classes)  # modification\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, self.norm))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def embed(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "def ResNet18BN_AP(channel, num_classes):\n",
        "    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n",
        "\n",
        "def ResNet18_AP(channel, num_classes):\n",
        "    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes)\n",
        "\n",
        "\n",
        "''' ResNet '''\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.norm = norm\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.norm = norm\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.norm = norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.classifier = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, self.norm))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def embed(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18BN(channel, num_classes):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n",
        "\n",
        "def ResNet18(channel, num_classes):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes)\n",
        "\n",
        "def ResNet34(channel, num_classes):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], channel=channel, num_classes=num_classes)\n",
        "\n",
        "def ResNet50(channel, num_classes):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], channel=channel, num_classes=num_classes)\n",
        "\n",
        "def ResNet101(channel, num_classes):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], channel=channel, num_classes=num_classes)\n",
        "\n",
        "def ResNet152(channel, num_classes):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], channel=channel, num_classes=num_classes)"
      ]
    }
  ]
}