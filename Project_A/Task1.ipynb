{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WYMfvCNPwpm"
      },
      "source": [
        "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA8ppgB2P0aJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Union\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "builder = tfds.builder('mnist')\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 12\n",
        "NUM_CLASSES = 10  # 10 total classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2EFLQROP2R7"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynByMG_UP4A4"
      },
      "outputs": [],
      "source": [
        "# Load train and test splits.\n",
        "def preprocess(x):\n",
        "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
        "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
        "  return image, subclass_labels\n",
        "\n",
        "\n",
        "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
        "mnist_train = mnist_train.map(preprocess)\n",
        "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
        "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "mnist_test = tfds.load('mnist', split='test').cache()\n",
        "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H8k64hPUnMO"
      },
      "outputs": [],
      "source": [
        "print(mnist_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAZwfvW5P63q"
      },
      "source": [
        "# Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zINgDkA7P7BP"
      },
      "outputs": [],
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Build CNN teacher.\n",
        "def create_teacher_model():\n",
        "    cnn_model = tf.keras.Sequential()\n",
        "\n",
        "    # your code start from here for stpe 2\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,strides=1, padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=1,padding='same'))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3,strides=1, padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=2,padding='same'))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "    cnn_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "    cnn_model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
        "\n",
        "    return cnn_model\n",
        "\n",
        "# Build fully connected student.\n",
        "def create_student_model():\n",
        "    fc_model = tf.keras.Sequential()\n",
        "\n",
        "    # your code start from here for step 2\n",
        "    fc_model.add(tf.keras.layers.Flatten())\n",
        "    fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "    fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "    fc_model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
        "    \n",
        "    return fc_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JWGucyrQGav"
      },
      "source": [
        "# Teacher loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhzBP6ZLQJ57"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def compute_teacher_loss(model, images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  subclass_logits = model(images, training=True)\n",
        "\n",
        "  # Compute cross-entropy loss for subclasses.\n",
        "  \n",
        "\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "\n",
        "\n",
        "\n",
        "  return cross_entropy_loss_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8xkuH0QbOS"
      },
      "source": [
        "# Student loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDKia4gPQMIr"
      },
      "outputs": [],
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "# Hyperparameters for distillation (need to be tuned).\n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  \"\"\"Compute distillation loss.\n",
        "\n",
        "  This function computes cross entropy between softened logits and softened\n",
        "  targets. The resulting loss is scaled by the squared temperature so that\n",
        "  the gradient magnitude remains approximately constant as the temperature is\n",
        "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
        "  a neural network.\"\n",
        "\n",
        "  Args:\n",
        "    teacher_logits: A Tensor of logits provided by the teacher.\n",
        "    student_logits: A Tensor of logits provided by the student, of the same\n",
        "      shape as `teacher_logits`.\n",
        "    temperature: Temperature to use for distillation.\n",
        "\n",
        "  Returns:\n",
        "    A scalar Tensor containing the distillation loss.\n",
        "  \"\"\"\n",
        " # your code start from here for step 3\n",
        "  soft_targets = tf.nn.softmax(teacher_logits/temperature)\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "def compute_student_loss(student_model, teacher_model, images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  student_subclass_logits = student_model(images, training=True)\n",
        "\n",
        "  # Compute subclass distillation loss between student subclass logits and\n",
        "  # softened teacher subclass targets probabilities.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "\n",
        "  teacher_subclass_logits = teacher_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
        "\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=student_subclass_logits)\n",
        "\n",
        "  return ALPHA * cross_entropy_loss_value + (1- ALPHA) * distillation_loss_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1uyvurQ3w4"
      },
      "source": [
        "# Train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtoLbp8uQ4Vl"
      },
      "outputs": [],
      "source": [
        "from six import with_metaclass\n",
        "@tf.function\n",
        "def compute_num_correct(model, images, labels):\n",
        "  \"\"\"Compute number of correctly classified images in a batch.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Number of correctly classified images.\n",
        "  \"\"\"\n",
        "  class_logits = model(images, training=False)\n",
        "  return tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn, with_kd, teacher_model):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  # your code start from here for step 4\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  best_accuracy = 0\n",
        "\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Run training.\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images, labels in mnist_train:\n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "        if not with_kd:\n",
        "          loss_value = compute_loss_fn(model, images, labels)\n",
        "        else:\n",
        "          loss_value = compute_loss_fn(model, teacher_model, images, labels)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = builder.info.splits['test'].num_examples\n",
        "    for images, labels in mnist_test:\n",
        "      # your code start from here for step 4\n",
        "      correct_results,_,_ = compute_num_correct(model, images, labels)\n",
        "      num_correct += correct_results\n",
        "\n",
        "\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "    if (num_correct / num_total * 100) > best_accuracy:\n",
        "      best_accuracy = num_correct / num_total * 100\n",
        "    \n",
        "  return best_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQL1lJdaRPT1"
      },
      "source": [
        "# Training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AGHbyABRPz3"
      },
      "outputs": [],
      "source": [
        "# your code start from here for step 5 \n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
        "\n",
        "cnn_model = None\n",
        "fc_model = None\n",
        "cnn_model = create_teacher_model()\n",
        "fc_model = create_student_model()\n",
        "\n",
        "train_and_evaluate(cnn_model, compute_teacher_loss, with_kd=False, teacher_model=None)\n",
        "cnn_model.save(\"task1_teacher_model\")\n",
        "train_and_evaluate(fc_model, compute_student_loss, with_kd=True, teacher_model=cnn_model)\n",
        "fc_model.save(\"task1_student_model\")\n",
        "\n",
        "cnn_model = None\n",
        "fc_model = None\n",
        "cnn_model = create_teacher_model()\n",
        "fc_model = create_student_model()\n",
        "\n",
        "ALPHA = 0.9 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 8. #temperature hyperparameter\n",
        "\n",
        "train_and_evaluate(cnn_model, compute_teacher_loss, with_kd=False, teacher_model=None)\n",
        "cnn_model.save(\"task1_teacher_model\")\n",
        "train_and_evaluate(fc_model, compute_student_loss, with_kd=True, teacher_model=cnn_model)\n",
        "fc_model.save(\"task1_student_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMQF6EvQdQLa"
      },
      "outputs": [],
      "source": [
        "sk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj1N38fnRTNB"
      },
      "source": [
        "# Test accuracy vs. tempreture curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX4dbazrRWIz"
      },
      "outputs": [],
      "source": [
        "# your code start from here for step 6\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE_LIST = [1,2,4,16,32,64] #temperature hyperparameter list\n",
        "accuracy_list = []\n",
        "\n",
        "for temp_single in DISTILLATION_TEMPERATURE_LIST:\n",
        "  \n",
        "  print('current tempreature is ', temp_single)\n",
        "  DISTILLATION_TEMPERATURE = temp_single\n",
        "  best_accuracy = train_and_evaluate(fc_model, compute_student_loss, with_kd=True, teacher_model=cnn_model)\n",
        "  \n",
        "  # Run evaluation.\n",
        "  num_correct = 0\n",
        "  num_total = builder.info.splits['test'].num_examples\n",
        "  \n",
        "  for images, labels in mnist_test:\n",
        "    # your code start from here for step 4\n",
        "    correct_results,_,_ = compute_num_correct(fc_model, images, labels)\n",
        "    num_correct += correct_results\n",
        "\n",
        "  print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "      num_correct / num_total * 100))\n",
        "\n",
        "  accuracy_list.append(best_accuracy)#(num_correct / num_total * 100)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.xlabel(\"temperature hyperparameter list\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.plot(DISTILLATION_TEMPERATURE_LIST,accuracy_list,color='r',linewidth=1.0,linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNrH_1emRbGA"
      },
      "source": [
        "# Train student from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjospsxIRbQ6"
      },
      "outputs": [],
      "source": [
        "# Build fully connected student.\n",
        "fc_model_no_distillation = tf.keras.Sequential()\n",
        "\n",
        "# your code start from here for step 7\n",
        "fc_model_no_distillation.add(tf.keras.layers.Flatten())\n",
        "fc_model_no_distillation.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "fc_model_no_distillation.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "fc_model_no_distillation.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
        "\n",
        "\n",
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "def compute_plain_cross_entropy_loss(model, images, labels):\n",
        "  \"\"\"Compute plain loss for given images and labels.\n",
        "\n",
        "  For fair comparison and convenience, this function also performs a\n",
        "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  # your code start from here for step 7\n",
        "\n",
        "  student_subclass_logits = model(images, training=True)\n",
        "  cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=student_subclass_logits)\n",
        "  \n",
        "  return cross_entropy_loss\n",
        "\n",
        "train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss, with_kd=False, teacher_model=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq3JTpQ4RuhR"
      },
      "source": [
        "# Comparing the teacher and student model (number of of parameters and FLOPs) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V8GB2yRRuxF"
      },
      "outputs": [],
      "source": [
        "# your code start from here for step 8\n",
        "#!pip install keras_flops\n",
        "\n",
        "from keras_flops import get_flops\n",
        "\n",
        "teacher_model_flops = get_flops(cnn_model, batch_size=1)\n",
        "student_model_flops = get_flops(fc_model, batch_size=1)\n",
        "student_without_KD_model_flops = get_flops(fc_model_no_distillation, batch_size=1)\n",
        "\n",
        "\n",
        "print('The flops of teacher model is',teacher_model_flops)\n",
        "print('The flops of student model is',student_model_flops)\n",
        "print('The flops of fc_model_no_distillation model is',student_without_KD_model_flops)\n",
        "\n",
        "cnn_model.summary()\n",
        "fc_model.summary()\n",
        "fc_model_no_distillation.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjwJ5oziRvRn"
      },
      "source": [
        "# Implementing the state-of-the-art KD algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q10lybAFRvZt"
      },
      "outputs": [],
      "source": [
        "# your code start from here for step 5 \n",
        "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
        "\n",
        "# your code start from here for step 12\n",
        "# Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. \n",
        "# Improved knowledge distillation via teacher assistant. \n",
        "# In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5191–5198, 2020. \n",
        "# https://ojs.aaai.org/ index.php/AAAI/article/view/5963/5819\n",
        "\n",
        "# Build Teacher assistant.\n",
        "ta_model = tf.keras.Sequential()\n",
        "ta_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,strides=1, padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "ta_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=1,padding='same'))\n",
        "ta_model.add(tf.keras.layers.Flatten())\n",
        "ta_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "ta_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
        "ta_model.add(tf.keras.layers.Dropout(0.5))\n",
        "ta_model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
        "\n",
        "def compute_ta_or_student_loss(images, labels, learn_model, teach_model):\n",
        "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  student_subclass_logits = learn_model(images, training=True)\n",
        "\n",
        "  # Compute subclass distillation loss between student subclass logits and\n",
        "  # softened teacher subclass targets probabilities.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "\n",
        "  teacher_subclass_logits = teach_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
        "\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "\n",
        "  # your code start from here for step 3\n",
        "\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=student_subclass_logits)\n",
        "\n",
        "  return ALPHA * cross_entropy_loss_value + (1- ALPHA) * distillation_loss_value\n",
        "\n",
        "def train_and_evaluate_state_of_the_art(model, teach_model):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  # your code start from here for step 4\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Run training.\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images, labels in mnist_train:\n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "\n",
        "        loss_value = compute_ta_or_student_loss(images, labels, model, teach_model)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = builder.info.splits['test'].num_examples\n",
        "    for images, labels in mnist_test:\n",
        "      # your code start from here for step 4\n",
        "      correct_results,_,_ = compute_num_correct(model, images, labels)\n",
        "      num_correct += correct_results\n",
        "\n",
        "\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "\n",
        "# train teacher assistant model\n",
        "train_and_evaluate_state_of_the_art(ta_model, teach_model=cnn_model)\n",
        "ta_model.save(\"task1_ta_model\")\n",
        "\n",
        "# train student model from teacher assistant model\n",
        "train_and_evaluate_state_of_the_art(fc_model, teach_model=ta_model)\n",
        "ta_model.save(\"task1_student_from_ta_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ogd5wAUnMb"
      },
      "outputs": [],
      "source": [
        "ta_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dsOmtqdieIC"
      },
      "source": [
        "# (Optional) XAI method to explain models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0IMIFW8ilPO"
      },
      "outputs": [],
      "source": [
        "# your code start from here for step 13\n",
        "!pip install pillow\n",
        "from PIL import Image, ImageDraw\n",
        "def build_mask_randomly(h=7, w=7, H=224, W=224, p_1=0.5, resample=Image.BILINEAR):\n",
        "\n",
        "    assert H>h, 'Masks should be higher dimensions.'\n",
        "    assert W>w, 'Masks should be higher dimensions.'\n",
        "    mask=np.random.choice([0, 1], size=(h, w), p=[1-p_1, p_1])\n",
        "\n",
        "    # upsample\n",
        "    mask = Image.fromarray(mask*255.)\n",
        "    mask = mask.resize((H + h, W + w), resample=resample)\n",
        "    mask = np.array(mask)\n",
        "\n",
        "    # randomly crop mask to HxW\n",
        "    w_crop = np.random.randint(0,w+1)\n",
        "    h_crop = np.random.randint(0,h+1)\n",
        "    mask = mask[h_crop:H + h_crop, w_crop:W + w_crop]\n",
        "    # normalize between 0 and 1\n",
        "    mask /= np.max(mask)\n",
        "    return mask\n",
        "\n",
        "def RISE(img, model, class_index, N_MASKS=8000, H=28, W=28, C=1):\n",
        "\n",
        "    X = np.zeros(shape=(N_MASKS, H, W, C), dtype=np.float32)\n",
        "    masks = np.zeros((N_MASKS,H,W), dtype=np.float32)\n",
        "    for i in range(N_MASKS):\n",
        "        m =build_mask_randomly(H=H, W=W)\n",
        "        masks[i] = m\n",
        "        x = img.copy()\n",
        "        x[:, :, 0] *= m\n",
        "        X[i] = x\n",
        "    preds_masked = model.predict(X, verbose=0)\n",
        "    sum_mask = np.zeros(masks[0].shape, dtype=np.float32)\n",
        "\n",
        "    for i, mask in enumerate(masks):\n",
        "        m = mask * preds_masked[i, class_index]\n",
        "        sum_mask += m\n",
        "\n",
        "    sum_mask -= np.min(sum_mask)\n",
        "    sum_mask /= np.max(sum_mask)\n",
        "    return sum_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zos8avgaA0Ht"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "one_sample, = mnist_test.take(1)\n",
        "images, labels = one_sample\n",
        "\n",
        "import random\n",
        "random.seed(99)\n",
        "number = random.randint(0,255)\n",
        "test_image = tf.squeeze(images[number])\n",
        "test_label = np.argmax(labels[number])\n",
        "# plt.figure(figsize=(12,12))\n",
        "plt.imshow(test_image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"The inference of the image label is: {}\".format(test_label))\n",
        "plt.show()\n",
        "\n",
        "teacher_pre = np.argmax(tf.nn.softmax(cnn_model(tf.expand_dims(sample_image, axis=0))))\n",
        "student_with_KD_pre = np.argmax(tf.nn.softmax(fc_model(tf.expand_dims(sample_image, axis=0))))\n",
        "student_without_KD_pre = np.argmax(tf.nn.softmax(fc_model_no_distillation(tf.expand_dims(sample_image, axis=0))))\n",
        "print(\"teacher_pre:\",teacher_pre,\"\\nstudent_with_KD_pre:\",student_with_KD_pre,\"\\nstudent_without_KD_pre\",student_without_KD_pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwVfEU3aBq3H"
      },
      "outputs": [],
      "source": [
        "rise_teacher = RISE(images[number].numpy(), cnn_model, class_index=test_label, N_MASKS=8000)\n",
        "rise_teacher -= rise_teacher.min()\n",
        "rise_teacher /= rise_teacher.max()+1e-10\n",
        "rise_student_with_KD = RISE(images[number].numpy(), fc_model, class_index=test_label, N_MASKS=8000)\n",
        "rise_student_with_KD -= rise_student_with_KD.min()\n",
        "rise_student_with_KD /= rise_student_with_KD.max()+1e-10\n",
        "rise_student_without_KD = RISE(images[number].numpy(), fc_model_no_distillation, class_index=test_label, N_MASKS=8000)\n",
        "rise_student_without_KD -= rise_student_without_KD.min()\n",
        "rise_student_without_KD /= rise_student_without_KD.max()+1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFv39glVCCeg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jWrUAlCCnuv"
      },
      "outputs": [],
      "source": [
        "# Plot Three explanation map from different models\n",
        "# Teacher Model\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.subplot(3,2,1)\n",
        "plt.title(\"origin label: {}\".format(test_label))\n",
        "plt.imshow(test_image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(3,2,2)\n",
        "plt.title('Teacher mdoel map')\n",
        "plt.imshow(rise_teacher, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "# Student w/ KD Model\n",
        "plt.subplot(3,2,3)\n",
        "plt.title(\"origin label: {}\".format(test_label))\n",
        "plt.imshow(test_image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(3,2,4)\n",
        "plt.title('Student model with KD map')\n",
        "plt.imshow(rise_student_with_KD, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "# Student w/o KD Model\n",
        "plt.subplot(3,2,5)\n",
        "plt.title(\"origin label: {}\".format(test_label))\n",
        "plt.imshow(test_image)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(3,2,6)\n",
        "plt.title('Student without KD map')\n",
        "plt.imshow(rise_student_without_KD, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD4131qQC1YT"
      },
      "outputs": [],
      "source": [
        "# origin image and explanation map are merged\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.subplot(1,3,1)\n",
        "plt.title('Teacher')\n",
        "plt.imshow(test_image)\n",
        "plt.imshow(rise_teacher, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('Student model with KD')\n",
        "plt.imshow(test_image)\n",
        "plt.imshow(rise_student_with_KD, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Student model without KD')\n",
        "plt.imshow(test_image)\n",
        "plt.imshow(rise_student_without_KD, cmap='jet', alpha=0.5)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rarGxsN7C8af"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d03c764adb86e7fe7b108c5d30474c5c945bb6d3c7f241663d66ebfb5c284ebb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
